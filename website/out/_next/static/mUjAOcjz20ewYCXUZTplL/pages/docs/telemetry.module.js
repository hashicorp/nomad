(window.webpackJsonp=window.webpackJsonp||[]).push([[264],{Ecnf:function(e,a,t){"use strict";t.r(a),t.d(a,"default",(function(){return m}));var n=t("wx14"),r=t("Ff2n"),o=t("q1tI"),s=t.n(o),i=t("7ljp"),c=t("j1un"),l=(s.a.createElement,{}),b=Object(c.a)({layout:"docs",page_title:"Telemetry Overview",sidebar_title:"Telemetry",description:"Overview of runtime metrics available in Nomad along with monitoring and\nalerting.",__resourcePath:"docs/telemetry/index.mdx",__scans:{}});function m(e){var{components:a}=e,t=Object(r.a)(e,["components"]);return Object(i.b)(b,Object(n.a)({},l,t,{components:a,mdxType:"MDXLayout"}),Object(i.b)("h1",{className:"g-type-display-2"},Object(i.b)("a",Object(n.a)({parentName:"h1"},{className:"__permalink-h",href:"#telemetry-overview","aria-label":"telemetry overview permalink"}),"\xbb"),Object(i.b)("a",Object(n.a)({parentName:"h1"},{className:"__target-h",id:"telemetry-overview","aria-hidden":""})),"Telemetry Overview"),Object(i.b)("p",null,"The Nomad client and server agents collect a wide range of runtime metrics\nrelated to the performance of the system. Operators can use this data to gain\nreal-time visibility into their cluster and improve performance. Additionally,\nNomad operators can set up monitoring and alerting based on these metrics in\norder to respond to any changes in the cluster state."),Object(i.b)("p",null,"On the server side, leaders and\nfollowers have metrics in common as well as metrics that are specific to their\nroles. Clients have separate metrics for the host metrics and for\nallocations/tasks, both of which have to be ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"/docs/configuration/telemetry"}),"explicitly\nenabled"),". There are also runtime metrics that are common to\nall servers and clients."),Object(i.b)("p",null,"By default, the Nomad agent collects telemetry data at a ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"/docs/configuration/telemetry#collection_interval"}),"1 second\ninterval"),". Note that Nomad supports ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"/docs/telemetry/metrics#metric-types"}),"Gauges, counters and\ntimers"),"."),Object(i.b)("p",null,"There are three ways to obtain metrics from Nomad:"),Object(i.b)("ul",null,Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),"Query the ",Object(i.b)("a",Object(n.a)({parentName:"li"},{href:"/api-docs/metrics"}),"/metrics API endpoint")," to return metrics for\nthe current Nomad process (as of Nomad 0.7). This endpoint supports Prometheus\nformatted metrics."),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),"Send the USR1 signal to the Nomad process. This will dump the current\ntelemetry information to STDERR (on Linux)."),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),"Configure Nomad to automatically forward metrics to a third-party provider.")),Object(i.b)("p",null,"Nomad 0.7 added support for ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"/docs/telemetry/metrics#tagged-metrics"}),"tagged metrics"),", improving the\nintegrations with ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"/docs/configuration/telemetry#datadog"}),"DataDog")," and ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"/docs/configuration/telemetry#prometheus"}),"Prometheus"),".\nMetrics can also be forwarded to ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"/docs/configuration/telemetry#statsite"}),"Statsite"),",\n",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"/docs/configuration/telemetry#statsd"}),"StatsD"),", and ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"/docs/configuration/telemetry#circonus"}),"Circonus"),"."),Object(i.b)("h2",{className:"g-type-display-3"},Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__permalink-h",href:"#alerting","aria-label":"alerting permalink"}),"\xbb"),Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__target-h",id:"alerting","aria-hidden":""})),"Alerting"),Object(i.b)("p",null,"The recommended practice for alerting is to leverage the alerting capabilities\nof your monitoring provider. Nomad\u2019s intention is to surface metrics that enable\nusers to configure the necessary alerts using their existing monitoring systems\nas a scaffold, rather than to natively support alerting. Here are a few common\npatterns:"),Object(i.b)("ul",null,Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("p",{parentName:"li"},"Export metrics from Nomad to Prometheus using the ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/prometheus/statsd_exporter"}),"StatsD\nexporter"),", define ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/"}),"alerting rules")," in\nPrometheus, and use ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://prometheus.io/docs/alerting/alertmanager/"}),"Alertmanager")," for summarization and\nrouting/notifications (to PagerDuty, Slack, etc.). A similar workflow is\nsupported for ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://www.datadoghq.com/blog/monitoring-101-alerting/"}),"Datadog"),".")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("p",{parentName:"li"},"Periodically submit test jobs into Nomad to determine if your application\ndeployment pipeline is working end-to-end. This pattern is well-suited to\nbatch processing workloads.")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("p",{parentName:"li"},"Deploy Nagios on Nomad. Centrally manage Nomad job files and add the Nagios\nmonitor when a new Nomad job is added. When a job is removed, remove the\nNagios monitor. Map Consul alerts to the Nagios monitor. This provides a\njob-specific alerting system.")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("p",{parentName:"li"},"Write a script that looks at the history of each batch job to determine\nwhether or not the job is in an unhealthy state, updating your monitoring\nsystem as appropriate. In many cases, it may be ok if a given batch job fails\noccasionally, as long as it goes back to passing."))),Object(i.b)("h1",{className:"g-type-display-2"},Object(i.b)("a",Object(n.a)({parentName:"h1"},{className:"__permalink-h",href:"#key-performance-indicators","aria-label":"key performance indicators permalink"}),"\xbb"),Object(i.b)("a",Object(n.a)({parentName:"h1"},{className:"__target-h",id:"key-performance-indicators","aria-hidden":""})),"Key Performance Indicators"),Object(i.b)("p",null,"The sections below cover a number of important metrics"),Object(i.b)("h2",{className:"g-type-display-3"},Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__permalink-h",href:"#consensus-protocol-raft","aria-label":"consensus protocol raft permalink"}),"\xbb"),Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__target-h",id:"consensus-protocol-raft","aria-hidden":""})),"Consensus Protocol (Raft)"),Object(i.b)("p",null,"Nomad uses the Raft consensus protocol for leader election and state\nreplication. Spurious leader elections can be caused by networking issues\nbetween the servers or insufficient CPU resources. Users in cloud environments\noften bump their servers up to the next instance class with improved networking\nand CPU to stabilize leader elections. The ",Object(i.b)("inlineCode",{parentName:"p"},"nomad.raft.leader.lastContact")," metric\nis a general indicator of Raft latency which can be used to observe how Raft\ntiming is performing and guide the decision to upgrade to more powerful servers.\n",Object(i.b)("inlineCode",{parentName:"p"},"nomad.raft.leader.lastContact")," should not get too close to the leader lease\ntimeout of 500ms."),Object(i.b)("h2",{className:"g-type-display-3"},Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__permalink-h",href:"#federated-deployments-serf","aria-label":"federated deployments serf permalink"}),"\xbb"),Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__target-h",id:"federated-deployments-serf","aria-hidden":""})),"Federated Deployments (Serf)"),Object(i.b)("p",null,"Nomad uses the membership and failure detection capabilities of the Serf library\nto maintain a single, global gossip pool for all servers in a federated\ndeployment. An uptick in ",Object(i.b)("inlineCode",{parentName:"p"},"member.flap")," and/or ",Object(i.b)("inlineCode",{parentName:"p"},"msg.suspect")," is a reliable indicator\nthat membership is unstable."),Object(i.b)("h2",{className:"g-type-display-3"},Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__permalink-h",href:"#scheduling","aria-label":"scheduling permalink"}),"\xbb"),Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__target-h",id:"scheduling","aria-hidden":""})),"Scheduling"),Object(i.b)("p",null,"The following metrics allow an operator to observe changes in throughput at the\nvarious points in the scheduling process (evaluation, scheduling/planning, and\nplacement):"),Object(i.b)("ul",null,Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.broker.total_blocked")," - The number of blocked evaluations."),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.worker.invoke_scheduler.\\<type",">")," - The time to run the scheduler of\nthe given type."),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.plan.evaluate")," - The time to evaluate a scheduler Plan."),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.plan.submit")," - The time to submit a scheduler Plan."),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.plan.queue_depth")," - The number of scheduler Plans waiting to be\nevaluated.")),Object(i.b)("p",null,"Upticks in any of the above metrics indicate a decrease in scheduler throughput."),Object(i.b)("h2",{className:"g-type-display-3"},Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__permalink-h",href:"#capacity","aria-label":"capacity permalink"}),"\xbb"),Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__target-h",id:"capacity","aria-hidden":""})),"Capacity"),Object(i.b)("p",null,"The importance of monitoring resource availability is workload specific. Batch\nprocessing workloads often operate under the assumption that the cluster should\nbe at or near capacity, with queued jobs running as soon as adequate resources\nbecome available. Clusters that are primarily responsible for long running\nservices with an uptime requirement may want to maintain headroom at 20% or\nmore. The following metrics can be used to assess capacity across the cluster on\na per client basis:"),Object(i.b)("ul",null,Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.client.allocated.cpu")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.client.unallocated.cpu")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.client.allocated.disk")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.client.unallocated.disk")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.client.allocated.iops")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.client.unallocated.iops")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.client.allocated.memory")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.client.unallocated.memory"))),Object(i.b)("h2",{className:"g-type-display-3"},Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__permalink-h",href:"#task-resource-consumption","aria-label":"task resource consumption permalink"}),"\xbb"),Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__target-h",id:"task-resource-consumption","aria-hidden":""})),"Task Resource Consumption"),Object(i.b)("p",null,"The metrics listed ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"/docs/telemetry/metrics#allocation-metrics"}),"here")," can be used to track resource\nconsumption on a per task basis. For user facing services, it is common to alert\nwhen the CPU is at or above the reserved resources for the task."),Object(i.b)("h2",{className:"g-type-display-3"},Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__permalink-h",href:"#job-and-task-status","aria-label":"job and task status permalink"}),"\xbb"),Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__target-h",id:"job-and-task-status","aria-hidden":""})),"Job and Task Status"),Object(i.b)("p",null,"We do not currently surface metrics for job and task/allocation status, although\nwe will consider adding metrics where it makes sense."),Object(i.b)("h2",{className:"g-type-display-3"},Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__permalink-h",href:"#runtime-metrics","aria-label":"runtime metrics permalink"}),"\xbb"),Object(i.b)("a",Object(n.a)({parentName:"h2"},{className:"__target-h",id:"runtime-metrics","aria-hidden":""})),"Runtime Metrics"),Object(i.b)("p",null,"Runtime metrics apply to all clients and servers. The following metrics are\ngeneral indicators of load and memory pressure:"),Object(i.b)("ul",null,Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.runtime.num_goroutines")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.runtime.heap_objects")),Object(i.b)("li",Object(n.a)({parentName:"ul"},{className:"g-type-long-body"}),Object(i.b)("strong",{parentName:"li"},"nomad.runtime.alloc_bytes"))),Object(i.b)("p",null,"It is recommended to alert on upticks in any of the above, server memory usage\nin particular."))}m.isMDXComponent=!0},"LsJ/":function(e,a,t){(window.__NEXT_P=window.__NEXT_P||[]).push(["/docs/telemetry",function(){return t("Ecnf")}])}},[["LsJ/",0,1,2,4,3,5,6]]]);