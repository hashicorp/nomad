---
layout: docs
page_title: Stateful Deployments
description: |-
  Learn how Nomad handles Stateful Deployments.
---

# Stateful Deployments

<Note>
Stateful deployments support only dynamic host volumes. For CSI volumes, we have
<a href="/nomad/docs/job-specification/volume#per_alloc">per_alloc</a> property
that serves a similar purpose.
</Note>

Nomad stateful deployments allow users to effectively bind their jobs to host
[volumes]. On a lower level, this means taking Nomad's basic unit of workload
placement—the [allocation]—and making it possible for users to have allocations
that are ["sticky"] to volume IDs. Since stateful deployments only support
dynamic host volumes, using them effectively means binding a job to a particular
node.

Stateful Deployments work on a per-task-group basis, that is, each task group
can indicate that a host volume it requires must be "sticky." This is done by
setting the parameter in the jobspec:

```hcl
job "app" {
  group "example" {
    # ...
    volume "example" {
      type      = "host"
      source    = "ca-certificates"
      read_only = true
      sticky    = true
    }
    # ...
  }
}
```

If you set the volume to "sticky," during the deployment the scheduler
associates its ID with the task group that uses it. Stateful deployments require
that any allocations belonging to that task group will only be (re)placed on
nodes that have this volume ID available.

Scaling jobs with sticky volumes up results in more volume IDs claimed by the
task group, but scaling jobs down does not delete unused volumes, nor does it
touch the data present on them.

If the scheduler cannot find a node that has the right volume ID present
(perhaps because the node is down or disconnected), a blocked evaluation is
created.

[allocation]: /nomad/docs/glossary#allocation
["sticky"]: /nomad/docs/job-specification/volume#sticky
[volumes]: /nomad/docs/other-specifications/volume
